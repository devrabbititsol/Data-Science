{
    "collab_server" : "",
    "contents" : "library(tm)\nlibrary(wordcloud)\nlibrary(syuzhet) \nlibrary(ggplot2)\nlibrary(dplyr )\nlibrary(NLP)\nlibrary(twitteR)\nlibrary(ggmap)\nlibrary(plyr)\nlibrary(stringr)\nlibrary(RColorBrewer)\nlibrary(Rfacebook)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(leaflet)\nlibrary(R2WinBUGS)\nlibrary(gender)\nlibrary(qdap)\nlibrary(data.table)\nlibrary(scales)\nlibrary(lubridate)\nlibrary(ROAuth)\nlibrary(httr)\nlibrary(lubridate)\nlibrary(chron)\nlibrary(devtools)\ninstall_github(\"pablobarbera/NYU-AD-160J/NYU160J\")\nlibrary(NYU160J)\nlibrary(xts)\nlibrary(stringi)\nlibrary(SciencesPo)\nlibrary(RNeo4j)\nlibrary(xts)\n\n\n\napi_key<-\"KyYm719eKWCz15KBkJVA2G82g\"\n\napi_secret<-\"LE8EDB5qByktXujkyAj5dlI5R8u7fAE7t1wE1Yogmg0gzkj4Wz\"\n\naccess_token<-\"2380143139-QB6qWurXS1bGEqFNbfASCOULIUDymBPsIFYaw2m\"\n\naccess_token_secret<-\"obg5agkjQRUofLtuaFz1F71sDbSFR5sQOyT8lquBal88R\"\n\nsetup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)\n\ntweet <- searchTwitter(\"eBay\", n=1000,lang=\"en\")  \n\ntweets.text <- lapply(tweet, function(t)t$getText())\n\n\n# MLBWhiteSoxShop <- getUser(\"4826245472\")\n# location(MLBWhiteSoxShop)\n\n################ remove the Twitter handlers\nnohandles <- str_replace_all(tweets.text, \"@\\\\w+\", \"\")\n\n#############clean up the remaining text\nwordCorpus <- Corpus(VectorSource(nohandles))\nwordCorpus <- tm_map(wordCorpus, removePunctuation)\nwordCorpus <- tm_map(wordCorpus, content_transformer(tolower))\nwordCorpus <- tm_map(wordCorpus, removeWords, stopwords(\"english\"))\nwordCorpus <- tm_map(wordCorpus, removeWords, c(\"like\", \"video\"))\nwordCorpus <- tm_map(wordCorpus, stripWhitespace)\nwordCorpus <- tm_map(wordCorpus, stemDocument)\ndtm <- TermDocumentMatrix(wordCorpus)\n\nm <- as.matrix(dtm)\n\nv <- sort(rowSums(m),decreasing=TRUE)\n\nd <- data.frame(word = names(v),freq=v)\n\npal <- brewer.pal(9,\"YlGnBu\")\npal <- pal[-(1:4)]\nset.seed(123)\n\n\n\n############################let us move to sentiment analysis\nmysentiment<-get_nrc_sentiment((nohandles))\n\n# Get the sentiment score for each emotion\nmysentiment.positive =sum(mysentiment$positive)\nmysentiment.anger =sum(mysentiment$anger)\nmysentiment.anticipation =sum(mysentiment$anticipation)\nmysentiment.disgust =sum(mysentiment$disgust)\nmysentiment.fear =sum(mysentiment$fear)\nmysentiment.joy =sum(mysentiment$joy)\nmysentiment.sadness =sum(mysentiment$sadness)\nmysentiment.surprise =sum(mysentiment$surprise)\nmysentiment.trust =sum(mysentiment$trust)\nmysentiment.negative =sum(mysentiment$negative)\n\n\n###########DB Creation#########\ntwf<-twListToDF(tweet)\ndb <- dbConnect(SQLite(), dbname='textanalysis.sqlite')\ndbWriteTable(conn = db, name = \"twf\", value = twf, row.names = FALSE)\nhistory<-dbReadTable(db, 'twf')\n\n######getting the locations \nusers <- lookupUsers(twf$screenName)\n\nusers_df <- twListToDF(users)\n\ncities <-users_df$location\ncities[cities == \" \"] <- \"NULL\"\ncities<-na.omit(cities)\nlonglat<-geocode(cities)\nlonglat<-cbind(longlat,cities)\nlonglat<-na.omit(longlat)\nView(longlat)\n\n\n#df.20 <- quakes[1:20,]\n\n# Names<-na.omit(users_df$name)\n# Names<-data.frame(Names)\n# for(i in length(users_df$name))\n# {\n#   if(longlat1$latt[i]!= \" \")\n#     \n#   longlat$names[i]<-name[i]\n# }\n\n# FavoritesCount<-na.omit(users_df$favoritesCount)\n# FavoritesCount<-data.frame(FavoritesCount)\n\n\n\n\n# function score.sentiment\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\n{\n  \n  scores = laply(sentences,\n                 function(sentence, pos.words, neg.words)\n                 {\n                   # remove punctuation\n                   sentence = gsub(\"[[:punct:]]\", \"\", sentence)\n                   # remove control characters\n                   sentence = gsub(\"[[:cntrl:]]\", \"\", sentence)\n                   # remove digits?\n                   sentence = gsub('\\\\d+', '', sentence)\n                   \n                   # define error handling function when trying tolower\n                   tryTolower = function(x)\n                   {\n                     # create missing value\n                     y = NA\n                     # tryCatch error\n                     try_error = tryCatch(tolower(x), error=function(e) e)\n                     # if not an error\n                     if (!inherits(try_error, \"error\"))\n                       y = tolower(x)\n                     # result\n                     return(y)\n                   }\n                   # use tryTolower with sapply\n                   sentence = sapply(sentence, tryTolower)\n                   \n                   # split sentence into words with str_split (stringr package)\n                   word.list = str_split(sentence, \"\\\\s+\")\n                   words = unlist(word.list)\n                   \n                   # compare words to the dictionaries of positive & negative terms\n                   pos.matches = match(words, pos.words)\n                   neg.matches = match(words, neg.words)\n                   \n                   # get the position of the matched term or NA\n                   # we just want a TRUE/FALSE\n                   pos.matches = !is.na(pos.matches)\n                   neg.matches = !is.na(neg.matches)\n                   \n                   # final score\n                   score = sum(pos.matches) - sum(neg.matches)\n                   return(score)\n                 }, pos.words, neg.words, .progress=.progress )\n  \n  # data frame with scores for each sentence\n  scores.df = data.frame(text=sentences,score=scores)\n  return(scores.df)\n}\n\n##############################Top Tweet\n\ndd<-max(twf$retweetCount)\n\n\n\ndd<-data.frame(dd)\nsetnames(dd,old='dd',new='Retweetcount')\ndd1<-which(twf$retweetCount == max(twf$retweetCount))\ntoptweet<-twf$text[which(twf$retweetCount == max(twf$retweetCount))]\ntoptweet<-data.frame(toptweet)\n\n                                         \n                                         \n                                         \n#####################Getting the positive and negative words\n\npos = readLines(\"positive_words.txt\")\nneg = readLines(\"negative_words.txt\")\nebay_txt = sapply(tweet, function(t)t$getText())\n#calculate Sentiment score\n\nscores = score.sentiment(ebay_txt, pos, neg, .progress='text')\n\npos_sen<-scores[scores$score>0,]\nneg_sen<-scores[scores$score<0,]\nneu_sen<-scores[scores$score==0,]\n\n#Count positive,negative,netural sentences\npositive_count<-nrow(pos_sen)\nnegative_count<-nrow(neg_sen)\nneutral_count<-nrow(neu_sen)\n\npolarity_count<-rbind(positive_count,negative_count,neutral_count)\n\ncnt<-as.data.frame(polarity_count)\ncnt1<-data.frame(sum(cnt$V1))\n\npolarity<-rbind(\"positive_count\",\"negative_count\",\"neutral_count\")\n\n\ncnt<-cbind(polarity,cnt)\nsetnames(cnt, old =c('polarity','V1'), new =c('Responces','Total_Count'))\n\n#Write Positive, Negative sentences into seperate files\n\nwrite.csv(pos_sen,\"Positive.csv\")\n\nwrite.csv(neg_sen,\"negative.csv\")\n\nwrite.csv(neu_sen,\"neutral.csv\")\n\n\n\ncolors <- c(\"green\",\"red\",\"blue\")\n\n\n\n\n##############################Followers and Friends of Ebay\nuser1<-getUser('ebay')\nFollowerscount<-user1$getFollowersCount()\n\nRecenttop10frnds<-user1$getFriends(n=10)\nRecenttop10frnds<-twListToDF(Recenttop10frnds)\n\nFavouritescount<-user1$getFavouritesCount()\n\n\n\n\n\n########################WordCloud Creation for Twitter\n\ntweets.text <- lapply(tweet, function(t)t$getText())\nclass(tweets.text)               \n\n# remove retweet entities\n\nsome_txt = gsub(\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\", tweets.text )\n\n# remove at people\n\nsome_txt = gsub(\"@\\\\w+\", \"\", some_txt)\n\n# remove punctuation\n\nsome_txt = gsub(\"[[:punct:]]\", \"\", some_txt)\n\n# remove numbers\n\nsome_txt = gsub(\"[[:digit:]]\", \"\", some_txt)\n\n# remove html links\n\nsome_txt = gsub(\"http\\\\w+\", \"\", some_txt)\n\n# remove unnecessary spaces\n\nsome_txt = gsub(\"[ \\t]{2,}\", \"\", some_txt)\n\nsome_txt = gsub(\"^\\\\s+|\\\\s+$\", \"\", some_txt)\n\ntweets_df <- data.frame(some_txt)\n\nmycorpus <- Corpus(VectorSource(tweets_df$some_txt))\n\npal <- brewer.pal(9,\"YlGnBu\")\npal <- pal[-(1:4)]\nset.seed(123)\nwordcloud::wordcloud(words = mycorpus,scale=c(5,0.1),max.words=100,\n                     random.order=FALSE,rot.per=0.35, use.r.layout=FALSE, colors=pal)\n\n\n\n\n\n# ############ Aggregate of Tweets Per Second on Twitter\n# tweets.df<-twf$created\n# tweets.df<-as.data.frame(cbind(twf$created,1),StringsAsFactors=FALSE)\n# colnames(tweets.df)<-c(\"Time\",\"Frequency\")\n# tweets.df$Time <- as.POSIXct(tweets.df$Time,origin=Sys.Date())\n# by.sec <- cut.POSIXt(tweets.df$Time,\"hour\")\n# tweets.sec <- split(tweets.df, by.sec)\n# s<-sapply(tweets.sec,function(x)sum(as.integer(x$Frequency)))\n# #View(table(by.sec))\n# count_df<-as.data.frame(table(by.sec))\n# count_df$by.sec<-sapply(count_df$by.sec,substring,12,19)\n# \n# count_df$Time <- format(count_df$by.sec, format = \"%H:%M:%S\")\n# \n# count_df$Time <- as.POSIXct(count_df$Time, format = \"%H:%M:%S\",size=4)\n# \n\n\n################### tweets per hour\ntweets<-userTimeline(\"ebay\",n=1000)\ntweets.df<-twListToDF(tweets)\nView(tweets.df)\n########## Tweet Per sec\")\ntweets.df$format<- as.POSIXct(tweets.df$created,format =\"%d-%m-%Y%H:%M:%S\", tz=\"\") \ntweets.df$Uhrzeit <- sub(\".* \", \"\", tweets.df$format)\ntweets.df1<-as.data.frame(cbind(tweets.df$Uhrzeit,1),stringsAsFactors=FALSE)\ncolnames(tweets.df1)<-c(\"time\",\"freq\")\nx <- as.POSIXct(tweets.df1$time,\"%H:%M:%S\",tz=\"\")\n#tweets.df1$time) <- strptime(x=tweets.df1$time,format=\"%H:%M:%S\",tz=\"\")\nby.hour <- cut.POSIXt(x,\"hour\")\nView(table(by.hour))\ntweets.hour <- split(tweets.df1, by.hour)\ncount_hour<-as.data.frame((sapply(tweets.hour,function(x)sum(as.integer(x$freq)))))\ncount_hour <- cbind(newColName = rownames(count_hour), count_hour)\nrownames(count_hour) <- 1:nrow(count_hour)\ncolnames(count_hour)<-c(\"Time\",\"Freq\")\nView(count_hour)\nHours<-data.frame(\n  time=format(as.POSIXct(count_hour$Time, format=\"%Y-%m-%d %H:%M\"), format=\"%H:%M\")\n)\n#Hours<-as.numeric(as.POSIXct(count_hour$time))\ncount_hour<-data.frame(cbind(count_hour,Hours))\nTimeinHours<-data.frame(count_hour$Time)\nFrequency<-data.frame(count_hour$Freq)\n\nts=xts(rep(1,times=nrow(tweets.df)),tweets.df$created)\n\n\n\n#######Aggrigate of Tweets per daily#########\nts.sum_daily=apply.daily(ts,sum) \n\nsum.daily=data.frame(date=index(ts.sum_daily), coredata(ts.sum_daily))\n\ncolnames(sum.daily)=c('Daily','sum')\n\nView(sum.daily)\n\nggplot(sum.daily)+geom_line(aes(x=Daily,y=sum))\n\n\n\n#######Aggrigate of Tweets per weekly#########\nts.sum_weekly=apply.weekly(ts,sum) \n\nsum.weekly=data.frame(date=index(ts.sum_weekly), coredata(ts.sum_weekly))\n\ncolnames(sum.weekly)=c('weekly','sum')\nView(sum.weekly)\n\nggplot(sum.weekly)+geom_line(aes(x=weekly,y=sum))\n\n\n#########################Aggrigate of Tweets per monthly####\nts.sum_monthly=apply.monthly(ts,sum) \n\nsum.monthly=data.frame(date=index(ts.sum_monthly), coredata(ts.sum_monthly))\n\ncolnames(sum.monthly)=c('monthly','sum')\nView(sum.monthly)\n\nggplot(sum.monthly)+geom_line(aes(x=monthly,y=sum))\n\n\n\n\n\n######Aggrigate of Tweets per Quarterly###############\n\nts.sum_quarterly=apply.quarterly(ts,sum) \n\nsum.quarterly=data.frame(date=index(ts.sum_quarterly), coredata(ts.sum_quarterly))\n\ncolnames(sum.quarterly)=c('quarterly','sum')\n\nView(sum.quarterly)\n\nggplot(sum.quarterly)+geom_line(aes(x=quarterly,y=sum))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n########################## Face Book Analysis\n\ntoken <-\"EAACEdEose0cBAI0gJc4UWLDl4pe2fMaJ4V1F14Vz4GcHhpn2yrlvTCfX7lHnEbgiN8MNGkeQX0Ls3hHnUlWDzaA9Y1OALmKpgBDqhILGVwVfbyXL9cOTGI6iBvyPRv2bxZAPBQvuc8q1fUlEJX5gFZA125ljXnZBbk36ZBZBpoZCTvmKbWIVvF34vDoKGZC0xcZD\"\n\n\nfb_oauth <- fbOAuth(app_id=\"168337503648123\", app_secret=\"f978f4315dfa109427bfb43fbdf03c3e\",extended_permissions = TRUE)\n# Request posts\nsave(fb_oauth, file=\"fb_oauth\")\n\nload(\"fb_oauth\")\npages <- searchPages( string=\"eBay\", token=token, n=200)\n\npost<-data.frame(pages)\n\n# remove retweet entities\n\nsome_txt = gsub(\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\", post$about)\n\n# remove at people\n\nsome_txt = gsub(\"@\\\\w+\", \"\", some_txt)\n\n# remove punctuation\n\nsome_txt = gsub(\"[[:punct:]]\", \"\", some_txt)\n\n# remove numbers\n\nsome_txt = gsub(\"[[:digit:]]\", \"\", some_txt)\n\n# remove html links\n\nsome_txt = gsub(\"http\\\\w+\", \"\", some_txt)\n\n# remove unnecessary spaces\n\nsome_txt = gsub(\"[ \\t]{2,}\", \"\", some_txt)\n\nsome_txt = gsub(\"^\\\\s+|\\\\s+$\", \"\", some_txt)\n\nsome_txt <- sapply(some_txt,function(row) iconv(row, \"latin1\", \"ASCII\", sub=\"\"))\n\ntweets_df <- data.frame(some_txt)\n\n\ntwe<-na.omit(tweets_df)\n\n\nmycorpus <- Corpus(VectorSource(tweets_df$some_txt))\ndtm1 <- TermDocumentMatrix(mycorpus)\n\nm1 <- as.matrix(dtm1)\n\nv1<- sort(rowSums(m1),decreasing=TRUE)\n\nd1 <- data.frame(word = names(v),freq=v)\n\n\n\n\n\n############################### Gender Analysis\n\n\n\npost<-searchPages(\"ebay\",token,n=100)\nposts<-post$id\n\n\nuser <- getUsers(posts,token=token)\nuser<-data.frame(user)\ngender <- user$gender\ng<-post$name\ng1<-findGivenNames(g)\ng2<-data.frame(g1)\n\nml<-subset(g2, gender==\"male\")\nfe<-subset(g2, gender==\"female\")\nx<-nrow(ml)\ny<-nrow(fe)\nslices <- data.frame(c(x,y))\nsetnames(slices, old = 'c.x..y.', new ='Gender_pct')\ngender1<-as.character(c('Male','Female'))\nslices<-data.frame(cbind(slices,gender1))\n\n\n\n\n#################### Number of Likes On ebay Monthly on Facebook\n\n\n\n\nformatFbDate <- function(datestring, format=\"datetime\") {\n  if (format==\"datetime\"){\n    date <- as.POSIXct(datestring, format = \"%Y-%m-%dT%H:%M:%S+0000\", tz = \"GMT\")    \n  }\n  if (format==\"date\"){\n    date <- as.Date(datestring, format = \"%Y-%m-%dT%H:%M:%S+0000\", tz = \"GMT\")   \n  }\n  return(date)\n}\n\npage$datetime = formatFbDate(page$created_time)\nresults = aggregateMetric(page, metric=\"likes\")\nresults1<-results\nMonths<-c(\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\",\"Jan\",\"Feb\",\"Mar\",\"Apr\")\nresults1<-cbind(results1,Months)\nsetnames(results1, old = 'x', new ='Total_Likes')\n\n\n\n\n################### Creation of wordcloud \n\npal <- brewer.pal(9,\"YlGnBu\")\npal <- pal[-(1:4)]\nset.seed(200)\nwordcloud::wordcloud(words = mycorpus,max.words=100,\n                     random.order=FALSE,rot.per=0.35, use.r.layout=FALSE, colors=pal)\n\n\n\n\n####################let us move to sentiment analysis\nmysentiment<-get_nrc_sentiment((some_txt))\n\n# Get the sentiment score for each emotion\nmysentiment.positive =sum(mysentiment$positive)\nmysentiment.anger =sum(mysentiment$anger)\nmysentiment.anticipation =sum(mysentiment$anticipation)\nmysentiment.disgust =sum(mysentiment$disgust)\nmysentiment.fear =sum(mysentiment$fear)\nmysentiment.joy =sum(mysentiment$joy)\nmysentiment.sadness =sum(mysentiment$sadness)\nmysentiment.surprise =sum(mysentiment$surprise)\nmysentiment.trust =sum(mysentiment$trust)\nmysentiment.negative =sum(mysentiment$negative)\n\n\nposneg<-data.frame(cbind(mysentiment,mysentiment.positive,mysentiment.anger,mysentiment.anticipation,mysentiment.disgust,mysentiment.fear,mysentiment.joy,mysentiment.sadness,mysentiment.trust,mysentiment.negative))\n\n\n#################Location wise post on facebook\ncities1<-post$city\ncitie1s[cities1 == \" \"] <- \"NULL\"\ncities1<-na.omit(cities1)\nlonglat1<-geocode(cities1)\nlonglat1<-cbind(longlat,cities1)\nlonglat1<-na.omit(longlat1)\nView(longlat1)\n\n#quakes1 <- quakes[1:10,]\n\n\n##########number of replied tweets\nggplot(twf, aes(factor(!is.na(replyToSID)))) +\n  geom_bar(fill = \"midnightblue\") +\n  theme(legend.position=\"none\", axis.title.x = element_blank()) +\n  ylab(\"Number of tweets\") +\n  ggtitle(\"Replied Tweets\") +\n  scale_x_discrete(labels=c(\"Not in reply\", \"Replied tweets\"))\n\n\nrepliednotrplyed<-data.frame(twf$replyToSID)\nreplied<-data.frame(na.omit(repliednotrplyed))\n#notrpiled<-data.frame(is.na(repliednotrplyed))\n# i<-1\n# for(i in notrpiled$twf.replyToSID==\"TRUE\")\n# {\n#   \n#     count[i]<-i\n# }\n\n#not<-data.frame(factor(!is.na(twf$replyToSID)))\n\n\n\n##############COUNTRYWISE POSTS COUNT\nsf<-searchPages(\"ebay\",token)\n\n\n\nsf %>%\n  group_by(country,city,longitude,latitude) %>%\n  summarise(count=n() ) %>%\n  arrange(desc(count) )-> cou\ncou1<-na.omit(cou)\nView(cou1)\ncou2<-aggregate(count~country,data=cou1,sum,na.rm=TRUE)\nct<-data.frame(sf$country)\nli<-data.frame(sf$likes)\nctli<-cbind(ct,li)\nctli<-na.omit(ctli)\n\n\nctli1<-aggregate(sf.likes~sf.country,data=ctli,sum,na.rm=TRUE)\nctli1 <- ctli1[-c(37,38, 39, 40,41), ]\ncountry_count_likes<-data.frame(cbind(ctli1,cou2))\nsetnames(country_count_likes,old = 'sf.likes',new='NumberOfLikes')\n\n\n\n#######likes,comments,shares\npage1<- getPage(\"ebay\", since = as.character(Sys.Date()-7), until = as.character(Sys.Date()), token, n = 100)\n\npage1$time <- as.POSIXct(page1$created_time,origin=\"Sys.Date()\")\n\n lastweeklikes<-data.frame(sum(page1$likes_count))\n lastweekcomments<-data.frame(sum(page1$comments_count))\n lastweekshares<-data.frame(sum(page1$shares_count))\nlastweekposts<-data.frame(count(page1$id))\n lastweekposts1<-data.frame(sum(lastweekposts$freq))\n\ncountries <- map.where(database=\"world\", x=longlat$lon, y=longlat$lat)\ncountries1<-data.frame((sort(table(countries), decreasing=TRUE)))\n\n\n\n\n\n",
    "created" : 1494334980396.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3796924197",
    "id" : "2BF355F2",
    "lastKnownWriteTime" : 1494335435,
    "last_content_update" : 1494335435766,
    "path" : "C:/Users/Nagadurga/Desktop/social_media analysis/socialmedia/social.R",
    "project_path" : "social.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}