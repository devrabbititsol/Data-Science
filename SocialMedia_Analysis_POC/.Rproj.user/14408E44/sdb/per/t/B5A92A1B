{
    "collab_server" : "",
    "contents" : "library(tm)\nlibrary(wordcloud)\nlibrary(syuzhet) \nlibrary(ggplot2)\nlibrary(dplyr )\nlibrary(NLP)\nlibrary(twitteR)\nlibrary(ggmap)\nlibrary(plyr)\nlibrary(stringr)\nlibrary(RColorBrewer)\nlibrary(Rfacebook)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(leaflet)\nlibrary(R2WinBUGS)\nlibrary(gender)\nlibrary(qdap)\nlibrary(data.table)\nlibrary(scales)\nlibrary(lubridate)\nlibrary(ROAuth)\nlibrary(httr)\nlibrary(lubridate)\nlibrary(chron)\nlibrary(devtools)\n# install_github(\"pablobarbera/NYU-AD-160J/NYU160J\")\nlibrary(NYU160J)\nlibrary(xts)\nlibrary(stringi)\nlibrary(SciencesPo)\nlibrary(RNeo4j)\nlibrary(xts)\nlibrary(DT)\n\n\n\n\napi_key<-\"KyYm719eKWCz15KBkJVA2G82g\"\napi_secret<-\"LE8EDB5qByktXujkyAj5dlI5R8u7fAE7t1wE1Yogmg0gzkj4Wz\"\naccess_token<-\"2380143139-QB6qWurXS1bGEqFNbfASCOULIUDymBPsIFYaw2m\"\naccess_token_secret<-\"obg5agkjQRUofLtuaFz1F71sDbSFR5sQOyT8lquBal88R\"\nsetup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)\ntweet <- searchTwitter(\"eBay\", n=1000,lang=\"en\")  \ntweets.text <- lapply(tweet, function(t)t$getText())\n\n################ remove the Twitter handlers\nnohandles <- str_replace_all(tweets.text, \"@\\\\w+\", \"\")\n\n#############clean up the remaining text\nwordCorpus <- Corpus(VectorSource(nohandles))\nwordCorpus <- tm_map(wordCorpus, removePunctuation)\nwordCorpus <- tm_map(wordCorpus, content_transformer(tolower))\nwordCorpus <- tm_map(wordCorpus, removeWords, stopwords(\"english\"))\nwordCorpus <- tm_map(wordCorpus, removeWords, c(\"like\", \"video\"))\nwordCorpus <- tm_map(wordCorpus, stripWhitespace)\nwordCorpus <- tm_map(wordCorpus, stemDocument)\ndtm <- TermDocumentMatrix(wordCorpus)\n\nm <- as.matrix(dtm)\n\nv <- sort(rowSums(m),decreasing=TRUE)\n\nd <- data.frame(word = names(v),freq=v)\n\n############################let us move to sentiment analysis\nmysentiment<-get_nrc_sentiment((nohandles))\n\n# Get the sentiment score for each emotion\nmysentiment.positive =sum(mysentiment$positive)\nmysentiment.anger =sum(mysentiment$anger)\nmysentiment.anticipation =sum(mysentiment$anticipation)\nmysentiment.disgust =sum(mysentiment$disgust)\nmysentiment.fear =sum(mysentiment$fear)\nmysentiment.joy =sum(mysentiment$joy)\nmysentiment.sadness =sum(mysentiment$sadness)\nmysentiment.surprise =sum(mysentiment$surprise)\nmysentiment.trust =sum(mysentiment$trust)\nmysentiment.negative =sum(mysentiment$negative)\n\nsentiment <- cbind.data.frame(mysentiment.positive,mysentiment.anger,mysentiment.anticipation,mysentiment.disgust,\n                              mysentiment.fear,mysentiment.joy,mysentiment.sadness,mysentiment.surprise,\n                              mysentiment.trust,mysentiment.negative)\n\n\n\nsentiment<-data.frame(rbind(mysentiment.positive,mysentiment.anger,mysentiment.anticipation,mysentiment.disgust,mysentiment.fear,mysentiment.joy,mysentiment.sadness,mysentiment.trust,mysentiment.negative))\n\nsentiname<-c(\"Positive\",\"Anger\",\"Anticipation\",\"Disgust\",\"Fear\",\"joy\",\"Sadness\",\"Trust\",\"Negative\")\n\nsentiment<-cbind.data.frame(sentiname,sentiment)\n\ncolnames(sentiment)<-c(\"Emotion\",\"value\")\n\n\n###########DB Creation#########\n\ntwf<-twListToDF(tweet)\n\n######getting the locations \nusers <- lookupUsers(twf$screenName)\n\nusers_df <- twListToDF(users)\n\ncities <-users_df$location\ncities[cities == \" \"] <- \"NULL\"\ncities<-na.omit(cities)\nlonglat<-geocode(cities)\nlonglat<-cbind(longlat,cities)\nlonglat<-na.omit(longlat)\n\n# function score.sentiment\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\n{\n  \n  scores = laply(sentences,\n                 function(sentence, pos.words, neg.words)\n                 {\n                   # remove punctuation\n                   sentence = gsub(\"[[:punct:]]\", \"\", sentence)\n                   # remove control characters\n                   sentence = gsub(\"[[:cntrl:]]\", \"\", sentence)\n                   # remove digits?\n                   sentence = gsub('\\\\d+', '', sentence)\n                   \n                   # define error handling function when trying tolower\n                   tryTolower = function(x)\n                   {\n                     # create missing value\n                     y = NA\n                     # tryCatch error\n                     try_error = tryCatch(tolower(x), error=function(e) e)\n                     # if not an error\n                     if (!inherits(try_error, \"error\"))\n                       y = tolower(x)\n                     # result\n                     return(y)\n                   }\n                   # use tryTolower with sapply\n                   sentence = sapply(sentence, tryTolower)\n                   \n                   # split sentence into words with str_split (stringr package)\n                   word.list = str_split(sentence, \"\\\\s+\")\n                   words = unlist(word.list)\n                   \n                   # compare words to the dictionaries of positive & negative terms\n                   pos.matches = match(words, pos.words)\n                   neg.matches = match(words, neg.words)\n                   \n                   # get the position of the matched term or NA\n                   # we just want a TRUE/FALSE\n                   pos.matches = !is.na(pos.matches)\n                   neg.matches = !is.na(neg.matches)\n                   \n                   # final score\n                   score = sum(pos.matches) - sum(neg.matches)\n                   return(score)\n                 }, pos.words, neg.words, .progress=.progress )\n  \n  # data frame with scores for each sentence\n  scores.df = data.frame(text=sentences,score=scores)\n  return(scores.df)\n}\n\n##############################Top Tweet\n  \n  dd<-max(twf$retweetCount)\n  \n  dd<-data.frame(dd)\n  setnames(dd,old='dd',new='Retweetcount')\n  dd1<-which(twf$retweetCount == max(twf$retweetCount))\n  toptweet<-twf$text[which(twf$retweetCount == max(twf$retweetCount))]\n  toptweet<-data.frame(toptweet)\n  \n  #####################Getting the positive and negative words\n  \n  pos = readLines(\"positive_words.txt\")\n  neg = readLines(\"negative_words.txt\")\n  tag_txt = sapply(tweet, function(t)t$getText())\n  #calculate Sentiment score\n  \n  scores = score.sentiment(tag_txt, pos, neg, .progress='text')\n  pos_sen<-scores[scores$score>0,]\n  \n  neg_sen<-scores[scores$score<0,]\n  neu_sen<-scores[scores$score==0,]\n  \n  #Count positive,negative,netural sentences\n  positive_count<-nrow(pos_sen)\n  negative_count<-nrow(neg_sen)\n  neutral_count<-nrow(neu_sen)\n  \n  \n  #Perecentages positive,\n  Positive_per<-(positive_count*100)/nrow(twf)\n  negative_per<-(negative_count*100)/nrow(twf)\n  neutral_per<- (neutral_count*100)/nrow(twf)\n  \n  polarity_count<-rbind(positive_count,negative_count,neutral_count)\n  cnt<-as.data.frame(polarity_count)\n  cnt1<-data.frame(sum(cnt$V1))\n  \n  polarity<-rbind(\"positive_count\",\"negative_count\",\"neutral_count\")\n  \n  cnt<-cbind(polarity,cnt)\n  setnames(cnt, old =c('polarity','V1'), new =c('Responces','Total_Count'))\n \n   polarity_per<-rbind(Positive_per,negative_per,neutral_per)\n\n    polarity_per <- cbind(newColName = rownames(polarity_per),polarity_per)\n  \n  rownames(polarity_per) <- 1:nrow(polarity_per)\n  \n \n colnames(polarity_per)<-c(\"polarity\",\"percentage\")\n\n polarity_per <-as.data.frame(as.matrix(polarity_per))\n  polarity_per$percentage<-as.numeric(as.character(polarity_per$percentage))\n\n  ##############################Followers and Friends of Ebay\n  user1<-getUser('ebay')\n  Followerscount<-user1$getFollowersCount()\n  \n  Recenttop10frnds<-user1$getFriends(n=10)\n  Recenttop10frnds<-twListToDF(Recenttop10frnds)\n  Favouritescount<-user1$getFavouritesCount()\n  Retweetscount<-sum(twf$retweetCount)\n  colnames(Recenttop10frnds)<-c(\"name\",\"location\",\"Followingcount\",\"Followerscount\",\"Likescount\")\n  Recenttop10frnds<-setDT(Recenttop10frnds)\n  \n  ########################WordCloud Creation for Twitter\n  \n  tweets.text <- lapply(tweet, function(t)t$getText())\n    \n  # remove retweet entities\n  some_txt = gsub(\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\",\"\", tweets.text )\n  \n  # remove at people\n  some_txt = gsub(\"@\\\\w+\", \"\", some_txt)\n  \n  # remove punctuation\n  some_txt = gsub(\"[[:punct:]]\", \"\", some_txt)\n  \n  # remove numbers\n  some_txt = gsub(\"[[:digit:]]\", \"\", some_txt)\n  \n  # remove html links\n  some_txt = gsub(\"http\\\\w+\", \"\", some_txt)\n  \n  # remove unnecessary spaces\n  some_txt = gsub(\"[ \\t]{2,}\", \"\", some_txt)\n  some_txt = gsub(\"^\\\\s+|\\\\s+$\", \"\", some_txt)\n  tweets_df <- data.frame(some_txt)\n  mycorpus <- Corpus(VectorSource(tweets_df$some_txt))\n  \n  pal <- brewer.pal(9,\"YlGnBu\")\n  pal <- pal[-(1:4)]\n  set.seed(123)\n  wordcloud::wordcloud(words = mycorpus,scale=c(5,0.1),max.words=100,\n                       random.order=FALSE,rot.per=0.35, use.r.layout=FALSE, colors=pal)\n  \n    ############ Aggregate of Tweets Per Second on Twitter\n  tweets.df<-twf$created\n  tweets.df<-as.data.frame(cbind(twf$created,1),StringsAsFactors=FALSE)\n  colnames(tweets.df)<-c(\"Time\",\"Frequency\")\n  tweets.df$Time <- as.POSIXct(tweets.df$Time,origin=Sys.Date())\n  by.sec <- cut.POSIXt(tweets.df$Time,\"min\")\n  tweets.sec <- split(tweets.df, by.sec)\n  s<-sapply(tweets.sec,function(x)sum(as.integer(x$Frequency)))\n  #View(table(by.sec))\n  count_df<-as.data.frame(table(by.sec))\n  count_df$by.sec<-sapply(count_df$by.sec,substring,12,19)\n  \n  count_df$Time <- format(count_df$by.sec, format = \"%H:%M:%S\")\n  \n  count_df$Time <- as.POSIXct(count_df$Time, format = \"%H:%M:%S\",size=4)\n  \n  count_df$Time<-as.character(count_df$Time)\n  \n  ################### tweets per hour\n  tweets<-userTimeline(\"ebay\",n=1000)\n  tweets.df<-twListToDF(tweets)\n  \n  ########## Tweet Per sec\")\n  tweets.df$format<- as.POSIXct(tweets.df$created,format =\"%d-%m-%Y%H:%M:%S\", tz=\"\") \n  tweets.df$Uhrzeit <- sub(\".* \", \"\", tweets.df$format)\n  tweets.df1<-as.data.frame(cbind(tweets.df$Uhrzeit,1),stringsAsFactors=FALSE)\n  colnames(tweets.df1)<-c(\"time\",\"freq\")\n  x <- as.POSIXct(tweets.df1$time,\"%H:%M:%S\",tz=\"\")\n  #tweets.df1$time) <- strptime(x=tweets.df1$time,format=\"%H:%M:%S\",tz=\"\")\n  by.hour <- cut.POSIXt(x,\"hour\")\n  \n  tweets.hour <- split(tweets.df1, by.hour)\n  count_hour<-as.data.frame((sapply(tweets.hour,function(x)sum(as.integer(x$freq)))))\n  count_hour <- cbind(newColName = rownames(count_hour), count_hour)\n  rownames(count_hour) <- 1:nrow(count_hour)\n  colnames(count_hour)<-c(\"Time\",\"Freq\")\n  \n  Hours<-data.frame(\n    time=format(as.POSIXct(count_hour$Time, format=\"%Y-%m-%d %H:%M\"), format=\"%H:%M\"))\n  #Hours<-as.numeric(as.POSIXct(count_hour$time))\n  count_hour<-data.frame(cbind(count_hour,Hours))\n  TimeinHours<-data.frame(count_hour$Time)\n  Frequency<-data.frame(count_hour$Freq)\n  \n  ts=xts(rep(1,times=nrow(tweets.df)),tweets.df$created)\n  \n  \n  \n  #######Aggrigate of Tweets per daily#########\n  ts.sum_daily=apply.daily(ts,sum) \n  \n  sum.daily=data.frame(date=index(ts.sum_daily), coredata(ts.sum_daily))\n  \n  colnames(sum.daily)=c('Daily','sum')\n  \n  ggplot(sum.daily)+geom_line(aes(x=Daily,y=sum))\n  \n  #######Aggrigate of Tweets per weekly#########\n  ts.sum_weekly=apply.weekly(ts,sum) \n  sum.weekly=data.frame(date=index(ts.sum_weekly), coredata(ts.sum_weekly))\n  colnames(sum.weekly)=c('weekly','sum')\n  ggplot(sum.weekly)+geom_line(aes(x=weekly,y=sum))\n  \n  #########################Aggrigate of Tweets per monthly####\n  ts.sum_monthly=apply.monthly(ts,sum) \n  sum.monthly=data.frame(date=index(ts.sum_monthly), coredata(ts.sum_monthly))\n  colnames(sum.monthly)=c('monthly','sum')\n  \n  ######Aggrigate of Tweets per Quarterly###############\n  \n  ts.sum_quarterly=apply.quarterly(ts,sum) \n  \n  sum.quarterly=data.frame(date=index(ts.sum_quarterly), coredata(ts.sum_quarterly))\n  \n  colnames(sum.quarterly)=c('quarterly','sum')\n  \n  ggplot(sum.quarterly)+geom_line(aes(x=quarterly,y=sum))\n  \n  ########################## Face Book Analysis\n  \n  token <-\"EAACEdEose0cBABPnuLEgFUFsE0QZABXxxoj6QCAUR97dsCBMn6RtrPOKC2P5mn8B6En6RiThw8ZAYs52Lv6hcgwyrWyNqxGga5TZB0eq9GRQhDCM2JvZC6u33OIE1E9XoYhgaYiBcrePfmvcgYGV4fjtGEljuOzG0ZB5PpmesZAs3VYBBNq3SlyDR87fYhZBeEZD\"\n  \n  fb_oauth <- fbOAuth(app_id=\"168337503648123\", app_secret=\"60a2bfbf835a58ee80138d4266ad9798\",extended_permissions = FALSE)\n  # Request posts\n  save(fb_oauth, file=\"fb_oauth\")\n  \n  load(\"fb_oauth\")\n  \n  pages <- searchPages(\"eBay\",token, n=100)\n  \n  post<-data.frame(pages)\n  \n  # remove retweet entities\n  \n  some_txt = gsub(\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\", post$about)\n  \n  # remove at people\n  \n  some_txt = gsub(\"@\\\\w+\", \"\", some_txt)\n  \n  # remove punctuation\n  \n  some_txt = gsub(\"[[:punct:]]\", \"\", some_txt)\n  \n  # remove numbers\n  \n  some_txt = gsub(\"[[:digit:]]\", \"\", some_txt)\n  \n  # remove html links\n  \n  some_txt = gsub(\"http\\\\w+\", \"\", some_txt)\n  \n  # remove unnecessary spaces\n  \n  some_txt = gsub(\"[ \\t]{2,}\", \"\", some_txt)\n  \n  some_txt = gsub(\"^\\\\s+|\\\\s+$\", \"\", some_txt)\n  \n  some_txt <- sapply(some_txt,function(row) iconv(row, \"latin1\", \"ASCII\", sub=\"\"))\n  \n  tweets_df <- data.frame(some_txt)\n  \n  twe<-na.omit(tweets_df)\n  \n  mycorpus <- Corpus(VectorSource(tweets_df$some_txt))\n  dtm1 <- TermDocumentMatrix(mycorpus)\n  \n  m1 <- as.matrix(dtm1)\n  \n  v1<- sort(rowSums(m1),decreasing=TRUE)\n  \n  d1 <- data.frame(word = names(v),freq=v)\n\n############################### Gender Analysis\n\n  require(Rfacebook)\n  library(gender)\n  library(genderizeR)\n  page<- getPage(\"ebay\", token, n = 200)\n  \n  post1<- getPost(\"185499393135_10156087605523136\", token, n = 1000, likes = TRUE,comments = TRUE)\n  library (plyr)\n  df2<- ldply (post1, data.frame)\n \n  gm<-df2$from_name\n  gm1<-findGivenNames(gm)\n  gm2<-data.frame(gm1)\n  \n \n  ml1<-subset(gm2, gender==\"male\")\n  fe1<-subset(gm2, gender==\"female\")\n  \n  \n  x1<-nrow(ml1)\n  y1<-nrow(fe1)\n  \n  slices <- c(x1,y1)\n\n#################### Number of Likes On ebay Monthly on Facebook\n\n  formatFbDate <- function(datestring, format=\"datetime\") {\n    if (format==\"datetime\"){\n      date <- as.POSIXct(datestring, format = \"%Y-%m-%dT%H:%M:%S+0000\", tz = \"GMT\")    \n    }\n    if (format==\"date\"){\n      date <- as.Date(datestring, format = \"%Y-%m-%dT%H:%M:%S+0000\", tz = \"GMT\")   \n    }\n    return(date)\n  }\n\n  page$datetime = formatFbDate(page$created_time)\n  results = aggregateMetric(page, metric=\"likes\")\n  results1<-results\n  Months<-c(\"mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\")\n  results1<-cbind(results1,Months)\n  setnames(results1, old = 'x', new ='Total_Likes')\n  \n  ################### Creation of wordcloud \n  \n  pal <- brewer.pal(9,\"YlGnBu\")\n  pal <- pal[-(1:4)]\n  set.seed(200)\n  wordcloud::wordcloud(words = mycorpus,max.words=100,\n                     random.order=FALSE,rot.per=0.35, use.r.layout=FALSE, colors=pal)\n\n####################let us move to sentiment analysis\n  mysentiment<-get_nrc_sentiment((some_txt))\n  \n  # Get the sentiment score for each emotion\n  mysentiment.positive =sum(mysentiment$positive)\n  mysentiment.anger =sum(mysentiment$anger)\n  mysentiment.anticipation =sum(mysentiment$anticipation)\n  mysentiment.disgust =sum(mysentiment$disgust)\n  mysentiment.fear =sum(mysentiment$fear)\n  mysentiment.joy =sum(mysentiment$joy)\n  mysentiment.sadness =sum(mysentiment$sadness)\n  mysentiment.surprise =sum(mysentiment$surprise)\n  mysentiment.trust =sum(mysentiment$trust)\n  mysentiment.negative =sum(mysentiment$negative)\n  \n  posneg<-cbind.data.frame(mysentiment.positive,mysentiment.anger,mysentiment.anticipation,mysentiment.disgust,\n                          mysentiment.fear,mysentiment.joy,mysentiment.sadness,mysentiment.trust,mysentiment.negative)\n\n  posneg<-data.frame(rbind(mysentiment.positive,mysentiment.anger,mysentiment.anticipation,mysentiment.disgust,mysentiment.fear,mysentiment.joy,mysentiment.sadness,mysentiment.trust,mysentiment.negative))\n  \n  posname<-c(\"Positive\",\"Anger\",\"Anticipation\",\"Disgust\",\"Fear\",\"joy\",\"Sadness\",\"Trust\",\"Negative\")\n  \n  posneg<-cbind.data.frame(posname,posneg)\n  \n  colnames(posneg)<-c(\"Emotion\",\"value\")\n\n  \n  #################Location wise post on facebook\n  cities1<-post$city\n  cities1[cities1 == \" \"] <- \"NULL\"\n  cities1<-na.omit(cities1)\n  longlat1<-geocode(cities1)\n  longlat1<-cbind(longlat1,cities1)\n  longlat1<-na.omit(longlat1)\n  View(longlat1)\n  \n  ##########number of replied tweets\n  ggplot(twf, aes(factor(!is.na(replyToSID)))) +\n    geom_bar(fill = \"midnightblue\") +\n    theme(legend.position=\"none\", axis.title.x = element_blank()) +\n    ylab(\"Number of tweets\") +\n    ggtitle(\"Replied Tweets\") +\n    scale_x_discrete(labels=c(\"Not in reply\", \"Replied tweets\"))\n\n  repliednotrplyed<-data.frame(twf$replyToSID)\n  replied<-data.frame(na.omit(repliednotrplyed))\n  \n  sf<-searchPages(\"ebay\",token)\n  \n    sf %>%\n      group_by(country,city,longitude,latitude) %>%\n      summarise(count=n() ) %>%\n      arrange(desc(count) )-> cou\n    cou1<-na.omit(cou)\n    View(cou1)\n    cou2<-aggregate(count~country,data=cou1,sum,na.rm=TRUE)\n    ct<-data.frame(sf$country)\n    li<-data.frame(sf$likes)\n    ctli<-cbind(ct,li)\n    ctli<-na.omit(ctli)\n    \n    ctli1<-aggregate(sf.likes~sf.country,data=ctli,sum,na.rm=TRUE)\n    ctli1 <- ctli1[-c(37,38, 39, 40,41), ]\n    country_count_likes<-data.frame(cbind(ctli1,cou2))\n    setnames(country_count_likes,old = 'sf.likes',new='NumberOfLikes')\n\n #######likes,comments,shares\n    page1<- getPage(\"ebay\", since = as.character(Sys.Date()-7), until = as.character(Sys.Date()), token, n = 100)\n    \n    page1$time <- as.POSIXct(page1$created_time,origin=\"Sys.Date()\")\n    \n    likesperweek <-data.frame(sum(page1$likes_count))\n   commentsperweek<-data.frame(sum(page1$comments_count))\n    sharesperweek<-data.frame(sum(page1$shares_count))\n    postsperweek<-data.frame(count(page1$id))\n    posts1perweek<-data.frame(sum(lastweekposts$freq))\n  library(maps)\n  countries <- map.where(database=\"world\", x=longlat$lon, y=longlat$lat)\n  countries1<-data.frame((sort(table(countries), decreasing=TRUE)))\n\n  library(Rfacebook)\n  library(dplyr)\n  library(lubridate)\n  library(stringr)\n  library(xts)\n  library(stringr)\n  library(xts)\n  \n  post24<-data.frame(page)\n  \n  posts24<-post24$id\n \n  library(stringr)\n  library(xts)\n  \n  post24$format <- strptime(post24$created_time,\"%FT%T\")\n  \n  post24$format<- as.POSIXct(post24$format,format =\"%d-%m-%Y%H:%M:%S\", tz=\"\") \n  \n  post24$hrmise <- sub(\".* \", \"\", post24$format)\n  \n  post.df1<-as.data.frame(cbind(post24$hrmise,1),stringsAsFactors=FALSE)\n  \n  colnames(post.df1)<-c(\"time\",\"freq\")\n  \n  x <- as.POSIXct(post.df1$time,\"%H:%M:%S\",tz=\"\")\n  \n  by.hour <- cut.POSIXt(x,\"hour\")\n  \n  View(table(by.hour))\n  \n  post.hour <- split(post.df1, by.hour)\n  \n  Post_count_hour<-as.data.frame((sapply(post.hour,function(x)sum(as.integer(x$freq)))))\n  \n \n  Post_count_hour <- cbind(newColName = rownames(Post_count_hour),Post_count_hour)\n  \n  rownames(Post_count_hour) <- 1:nrow(Post_count_hour)\n  \n  colnames(Post_count_hour)<-c(\"Time\",\"Freq\")\n  \n  #Post_count_hour$Time<-substr(Post_count_hour$Time,12,19)\n  \n  View(Post_count_hour)\n  \n  ######################################################### up date 25-5-2017\n  \n  \n  #######Aggrigate of Retweets Lastweek#########\n  \n  tweet <- userTimeline(\"eBay\", n=1000)  \n  \n  tweets.df<-twListToDF(tweets)\n  \n  tweets.df$format<- as.POSIXct(tweets.df$created,format =\"%d-%m-%Y%H:%M:%S\", tz=\"\") \n  ts1=xts(tweets.df$retweetCount,tweets.df$format)\n  ts1.sum_weekly=apply.weekly(ts1,sum) \n  sum.weekly=data.frame(date=index(ts1.sum_weekly), coredata(ts1.sum_weekly))\n  colnames(sum.weekly)=c('weekly','sum')\n  View(sum.weekly)\n  index<-nrow(sum.weekly)\n  RetweetcountWeek<-sum.weekly$sum[index]\n   \n  #########################Aggrigate of Twitter Likes  Last week####\n  \n  \n  ts=xts(tweets.df$favoriteCount,tweets.df$format)\n  ts.sum_weekly_likes=apply.weekly(ts,sum) \n  sum.weekly_likes=data.frame(date=index(ts.sum_weekly_likes), coredata(ts.sum_weekly_likes))\n  colnames(sum.weekly_likes)=c('weekly_likes','sum_likes')\n  View(sum.weekly_likes)\n  index<-nrow(sum.weekly_likes)\n  lastweeklikescount<-sum.weekly_likes$sum_likes[index]\n\n",
    "created" : 1496235588068.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1659405294",
    "id" : "B5A92A1B",
    "lastKnownWriteTime" : 1495719708,
    "last_content_update" : 1495719708,
    "path" : "C:/Users/ADMIN/Downloads/socialmedia/social.R",
    "project_path" : "social.R",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}